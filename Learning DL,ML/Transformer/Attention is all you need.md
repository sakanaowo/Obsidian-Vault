---
aliases:
  - Attention
tags:
  - transformer
  - CNN
  - RNN
  - vanishing-gradient
---
---
# 1. Intro
### ğŸ” Váº¥n Ä‘á» cáº§n giáº£i quyáº¿t

- CÃ¡c mÃ´ hÃ¬nh dá»‹ch mÃ¡y truyá»n thá»‘ng nhÆ° RNN vÃ  CNN cÃ³ nhiá»u háº¡n cháº¿:
    - **RNN xá»­ lÃ½ tuáº§n tá»±**, gÃ¢y khÃ³ khÄƒn trong viá»‡c táº­n dá»¥ng GPU song song.        
    - **CNN cÃ³ pháº¡m vi nhÃ¬n nháº­n cá»¥c bá»™**, khÃ´ng thá»ƒ há»c Ä‘Æ°á»£c má»‘i quan há»‡ xa trong cÃ¢u.
    - Attention Ä‘Ã£ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ cáº£i thiá»‡n RNN/CNN, nhÆ°ng váº«n chÆ°a táº­n dá»¥ng háº¿t tiá»m nÄƒng cá»§a nÃ³.

### âœ… Transformer mang Ä‘áº¿n Ä‘iá»u gÃ¬ má»›i?
- HoÃ n toÃ n dá»±a trÃªn **Self-Attention**, loáº¡i bá» hoÃ n toÃ n RNN/CNN.
- MÃ´ hÃ¬nh cÃ³ thá»ƒ **xá»­ lÃ½ song song toÃ n bá»™ cÃ¢u**, giÃºp huáº¥n luyá»‡n nhanh hÆ¡n.
- Hiá»‡u suáº¥t dá»‹ch ngÃ´n ngá»¯ tá»‘t hÆ¡n cÃ¡c mÃ´ hÃ¬nh cÅ©.
# 2. Background (Bá»‘i cáº£nh)
### ğŸ” Váº¥n Ä‘á» trong xá»­ lÃ½ chuá»—i

- **RNN:**
    
    - Há»c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« theo thá»© tá»±, nhÆ°ng khÃ³ há»c cÃ¡c quan há»‡ xa.
        
    - Dá»… gáº·p váº¥n Ä‘á» **vanishing gradient**, khiáº¿n viá»‡c há»c cÃ¡c phá»¥ thuá»™c dÃ i trá»Ÿ nÃªn kÃ©m hiá»‡u quáº£.
        
- **CNN:**
    
    - CÃ³ thá»ƒ xá»­ lÃ½ song song, nhÆ°ng chá»‰ táº­p trung vÃ o cÃ¡c vÃ¹ng cá»¥c bá»™, khÃ´ng thá»ƒ nhÃ¬n toÃ n bá»™ cÃ¢u má»™t lÃºc.
        

### âœ… Self-Attention kháº¯c phá»¥c ra sao?

- **Self-Attention cÃ³ thá»ƒ nhÃ¬n toÃ n bá»™ cÃ¢u cÃ¹ng má»™t lÃºc**.
    
- Dá»… dÃ ng má»Ÿ rá»™ng mÃ´ hÃ¬nh nhá» kháº£ nÄƒng xá»­ lÃ½ song song.
# 3. Model Architecture (Kiáº¿n trÃºc)
![](https://i.imgur.com/7ICFlKY.png)

### 3.1 **Encoder and Decoder Stacks**

ğŸ” Váº¥n Ä‘á»:
- CÃ¡c mÃ´ hÃ¬nh cÅ© pháº£i dÃ¹ng cÆ¡ cháº¿ Attention Ä‘á»ƒ liÃªn káº¿t Encoder vÃ  Decoder nhÆ°ng váº«n dá»±a trÃªn RNN/CNN.  

âœ… Giáº£i phÃ¡p:
- Transformer dÃ¹ng hoÃ n toÃ n **Self-Attention** vÃ  **Feed-Forward Networks (FFN)** trong cáº£ Encoder vÃ  Decoder.

### 3.2 **Attention**
![](https://i.imgur.com/Zyyp40c.png)


ğŸ” Váº¥n Ä‘á»:
- Trong má»™t cÃ¢u, khÃ´ng pháº£i tá»« nÃ o cÅ©ng quan trá»ng nhÆ° nhau.  

âœ… Giáº£i phÃ¡p:
- **Multi-Head Self-Attention** giÃºp mÃ´ hÃ¬nh cÃ³ thá»ƒ nhÃ¬n vÃ o nhiá»u pháº§n khÃ¡c nhau cá»§a cÃ¢u Ä‘á»“ng thá»i.  

### 3.3 **Position-wise Feed-Forward Networks**

ğŸ” Váº¥n Ä‘á»:
- CÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Attention cáº§n thÃªm má»™t lá»›p phi tuyáº¿n Ä‘á»ƒ tÄƒng Ä‘á»™ phá»©c táº¡p.  

âœ… Giáº£i phÃ¡p:
- Má»—i tá»« sau khi Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi Self-Attention sáº½ Ä‘i qua má»™t máº¡ng **Feed-Forward riÃªng biá»‡t**.
    

### 3.4 **Embeddings and Softmax**

ğŸ” Váº¥n Ä‘á»:

- Cáº§n Ã¡nh xáº¡ tá»« ngá»¯ thÃ nh vector sá»‘ Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ xá»­ lÃ½.  
    âœ… Giáº£i phÃ¡p:
    
- DÃ¹ng **Word Embeddings** vÃ  **Softmax** Ä‘á»ƒ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra.
    

### 3.5 **Positional Encoding**

ğŸ” Váº¥n Ä‘á»:

- Transformer khÃ´ng cÃ³ cÆ¡ cháº¿ tuáº§n tá»± nhÆ° RNN, nÃªn khÃ´ng biáº¿t thá»© tá»± tá»« trong cÃ¢u.  
    âœ… Giáº£i phÃ¡p:
    
- **Positional Encoding** giÃºp mÃ´ hÃ¬nh biáº¿t vá»‹ trÃ­ cá»§a tá»«ng tá»« trong cÃ¢u.
# 4. Why Self-Attention (Táº¡i sao sá»­ dá»¥ng Self-Attention)

# 5. Training

# 6. Result

# 7. Conclusion
